{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18d2d19b",
   "metadata": {},
   "source": [
    "# Cell-penetrating peptides (CPP) prediction\n",
    "\n",
    "This notebook focus on linear peptides with all natural amino acids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3726f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e8db3f5",
   "metadata": {},
   "source": [
    "# CPP924 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a4dc41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cpp924 = pd.read_csv('data/CPP924/train.csv')\n",
    "df_test_cpp924 = pd.read_csv('data/CPP924/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c573f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train_cpp924.is_cpp.values\n",
    "y_test = df_test_cpp924.is_cpp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8d0f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "def get_metrics(y_hat, y_test):\n",
    "    acc = accuracy_score(y_test, y_hat)\n",
    "    sn = recall_score(y_test, y_hat)\n",
    "    sp = recall_score(y_test, y_hat, pos_label=0)\n",
    "    mcc = matthews_corrcoef(y_test, y_hat)\n",
    "    auroc = roc_auc_score(y_test, y_hat)\n",
    "\n",
    "    print(f'Acc(%) \\t Sn(%) \\t Sp(%) \\t MCC \\t AUROC')\n",
    "    print(f'{acc*100:.2f}\\t{sn*100:.2f}\\t{sp*100:.2f}\\t{mcc:.3f}\\t{auroc:.3f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef2ea4ab",
   "metadata": {},
   "source": [
    "## Feature processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb2bd0e7",
   "metadata": {},
   "source": [
    "### Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd691b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from rdkit import Chem, rdBase, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from typing import List\n",
    "\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def fingerprints_from_smiles(smiles: List, size=2048):\n",
    "    \"\"\"\n",
    "        Create ECFP fingerprints of smiles, with validity check\n",
    "    \"\"\"\n",
    "    fps = []\n",
    "    valid_mask = []\n",
    "    for i, smile in enumerate(smiles):\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        valid_mask.append(int(mol is not None))\n",
    "        fp = fingerprints_from_mol(mol, size=size) if mol else np.zeros((1, size))\n",
    "        fps.append(fp)\n",
    "\n",
    "    fps = np.concatenate(fps, axis=0)\n",
    "    return fps, valid_mask\n",
    "\n",
    "\n",
    "def fingerprints_from_mol(molecule, radius=3, size=2048, hashed=False):\n",
    "    \"\"\"\n",
    "        Create ECFP fingerprint of a molecule\n",
    "    \"\"\"\n",
    "    if hashed:\n",
    "        fp_bits = AllChem.GetHashedMorganFingerprint(molecule, radius, nBits=size)\n",
    "    else:\n",
    "        fp_bits = AllChem.GetMorganFingerprintAsBitVect(molecule, radius, nBits=size)\n",
    "    fp_np = np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(fp_bits, fp_np)\n",
    "    return fp_np.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07976b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((733, 2048), (733,), (183, 2048), (183,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = True\n",
    "if train:\n",
    "    X_train = fingerprints_from_smiles(df_train_cpp924.smi)[0]\n",
    "    X_test = fingerprints_from_smiles(df_test_cpp924.smi)[0]\n",
    "\n",
    "    np.save('data/CPP924/X_train_fps.npy', X_train)\n",
    "    np.save('data/CPP924/X_test_fps.npy', X_test)\n",
    "else:\n",
    "    X_train = np.load('data/CPP924/X_train_fps.npy')\n",
    "    X_test = np.load('data/CPP924/X_test_fps.npy')\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0d5137a",
   "metadata": {},
   "source": [
    "### ESM-2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8fe3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "def load_esm2_model():\n",
    "    # Load ESM-2 model\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()  # disables dropout for deterministic results\n",
    "    return model, batch_converter, alphabet\n",
    "\n",
    "def get_esm_seq_representation(aa_seqs, batch_converter, model, alphabet):\n",
    "\n",
    "    data = [(f\"seq{id}\", seq) for id, seq in enumerate(aa_seqs)]\n",
    "    _, _, batch_tokens = batch_converter(data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][33]\n",
    "\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0)) # Take mean of non-pad tokens\n",
    "    \n",
    "    return torch.stack(sequence_representations).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69c4d892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((733, 1280), (733,), (183, 1280), (183,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = True\n",
    "if train:\n",
    "    model, batch_converter, alphabet = load_esm2_model()\n",
    "    X_train = get_esm_seq_representation(df_train_cpp924.aa_seq, batch_converter, model, alphabet)\n",
    "    X_test = get_esm_seq_representation(df_test_cpp924.aa_seq, batch_converter, model, alphabet)\n",
    "\n",
    "    np.save('data/CPP924/X_train_esm2.npy', X_train)\n",
    "    np.save('data/CPP924/X_test_esm2.npy', X_test)\n",
    "else:\n",
    "    X_train = np.load('data/CPP924/X_train_esm2.npy')\n",
    "    X_test = np.load('data/CPP924/X_test_esm2.npy')\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f7dbbc4",
   "metadata": {},
   "source": [
    "### SMILES BERT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a13502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "from models.bert import BERT\n",
    "from datasets.smiles_dataset import SmilesTokenizer\n",
    "from utils.utils import load_model\n",
    "\n",
    "def load_smi_bert_model(ckpt='results/pretrain/bak3/model_10_0.070.pt', device='cuda'):\n",
    "    tokenizer = SmilesTokenizer()\n",
    "    model = BERT(tokenizer, context_length=400, width=512, n_heads=8, n_layers=6)\n",
    "    model = load_model(model, ckpt, device)\n",
    "    model.eval()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def get_smi_embd(smi_encoder, smiles, device='cuda'):\n",
    "    smi_tokens_raw = smi_encoder.tokenizer.batch_encode(smiles).to(device)\n",
    "    # smi_tokens = smi_encoder.process_inputs(smi_tokens_raw)\n",
    "    smi_tokens = smi_encoder.process_batch(smi_tokens_raw)\n",
    "    batch_lens = (smi_tokens != smi_encoder.tokenizer.pad_token_id).sum(1)\n",
    "    smi_embd = smi_encoder.embed(smi_tokens)\n",
    "    smi_reps = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        smi_reps.append(smi_embd[i, 1 : tokens_len - 1].mean(0))\n",
    "    \n",
    "    return torch.stack(smi_reps)\n",
    "\n",
    "def encode_smi(smi_list, tokenizer, device, model):\n",
    "    with torch.no_grad():\n",
    "        output= get_smi_embd(model, smi_list, device='cuda')\n",
    "        embd = output.cpu().numpy() #.mean(axis=1)\n",
    "    return embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "81a327b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((733, 512), (733,), (183, 512), (183,))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = True\n",
    "if train:\n",
    "    # model, tokenizer, device = load_smi_bert_model(ckpt='results/pretrain/bak3/model_final_0.061.pt')  #79.78\t84.69\t74.12\t0.593\t0.794\n",
    "    # model, tokenizer, device = load_smi_bert_model(ckpt='results/pretrain/bak3/model_10_0.070.pt')  #77.05\t82.65\t70.59\t0.538\t0.766\n",
    "    # model, tokenizer, device = load_smi_bert_model(ckpt='results/pretrain/bak3/model_1_0.093.pt')  #75.41\t76.53\t74.12\t0.506\t0.753\n",
    "    model, tokenizer, device = load_smi_bert_model(ckpt='results/pretrain/bak1/model_3_0.109.pt')  #73.22\t76.53\t69.41\t0.461\t0.730\n",
    "\n",
    "    X_train = encode_smi(df_train_cpp924.smi, tokenizer, device, model)\n",
    "    X_test = encode_smi(df_test_cpp924.smi, tokenizer, device, model)\n",
    "\n",
    "    np.save('data/CPP924/X_train_smi_bert.npy', X_train)\n",
    "    np.save('data/CPP924/X_test_smi_bert.npy', X_test)\n",
    "else:\n",
    "    X_train = np.load('data/CPP924/X_train_smi_bert.npy')\n",
    "    X_test = np.load('data/CPP924/X_test_smi_bert.npy')\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f08286c",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd8b4000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((733, 512), (733,), (183, 512), (183,))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = ['fps', ]\n",
    "# features = ['esm2', ] \n",
    "features = ['smi_bert', ] \n",
    "# features = ['molebert', ] \n",
    "# features = ['esm2', 'fps', ]  \n",
    "# features = ['esm2', 'smi_bert', ]\n",
    "# features = ['esm2', 'fps', 'smi_bert']\n",
    "\n",
    "X_train_features = []\n",
    "X_test_features = []\n",
    "for feat in features:\n",
    "    if feat == 'esm2':\n",
    "        X_train = np.load('data/CPP924/X_train_esm2.npy')\n",
    "        X_test = np.load('data/CPP924/X_test_esm2.npy')\n",
    "        \n",
    "        X_train_features.append(X_train)\n",
    "        X_test_features.append(X_test)\n",
    "    elif feat == 'fps':\n",
    "        X_train = np.load('data/CPP924/X_train_fps.npy')\n",
    "        X_test = np.load('data/CPP924/X_test_fps.npy')\n",
    "\n",
    "        X_train_features.append(X_train)\n",
    "        X_test_features.append(X_test)\n",
    "    elif feat == 'smi_bert':\n",
    "        X_train = np.load('data/CPP924/X_train_smi_bert.npy')\n",
    "        X_test = np.load('data/CPP924/X_test_smi_bert.npy')\n",
    "\n",
    "        X_train_features.append(X_train)\n",
    "        X_test_features.append(X_test)\n",
    "    elif feat == 'molebert':\n",
    "        X_train = np.load('data/CPP924/X_train_molebert.npy')\n",
    "        X_test = np.load('data/CPP924/X_test_molebert.npy')\n",
    "\n",
    "        X_train_features.append(X_train)\n",
    "        X_test_features.append(X_test)\n",
    "    else:\n",
    "        raise ValueError(f'Feature {feat} not supported')\n",
    "\n",
    "X_train = np.concatenate(X_train_features, axis=1)\n",
    "X_test = np.concatenate(X_test_features, axis=1)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1edcfd8",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "514f5075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc(%) \t Sn(%) \t Sp(%) \t MCC \t AUROC\n",
      "73.22\t76.53\t69.41\t0.461\t0.730\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "get_metrics(y_hat, y_test)\n",
    "\n",
    "# Features      Acc(%) \t Sn(%) \t Sp(%) \t MCC \t AUROC\n",
    "# FPS:          86.89\t91.84\t81.18\t0.738\t0.865\n",
    "# ESM:          93.44\t95.92\t90.59\t0.869\t0.933 \n",
    "#               92.90\t95.92\t89.41\t0.858\t0.927\n",
    "#               93.44\t93.88\t92.94\t0.868\t0.934*\n",
    "# SMI:          90.16\t91.84\t88.24\t0.802\t0.900\n",
    "#               90.16\t90.82\t89.41\t0.802\t0.901\n",
    "#               91.80\t93.88\t89.41\t0.835\t0.916\n",
    "#               87.43\t92.86\t81.18\t0.749\t0.870\n",
    "# Mole-BERT:    51.91\t45.92\t58.82\t0.048\t0.524\n",
    "\n",
    "\n",
    "# ESM+FPS:      93.99\t94.90\t92.94\t0.879\t0.939\n",
    "#               91.80\t92.86\t90.59\t0.835\t0.917\n",
    "#               92.90\t94.90\t90.59\t0.857\t0.927\n",
    "#               94.54\t96.94\t91.76\t0.891\t0.944*\n",
    "#               93.44\t94.90\t91.76\t0.868\t0.933\n",
    "# ESM+SMI:      93.44\t95.92\t90.59\t0.869\t0.933\n",
    "#               92.35\t94.90\t89.41\t0.847\t0.922\n",
    "#               93.99\t95.92\t91.76\t0.879\t0.938\n",
    "\n",
    "# ESM+SMI+FPS:  93.99\t95.92\t91.76\t0.879\t0.938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff9713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
