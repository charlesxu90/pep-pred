{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18d2d19b",
   "metadata": {},
   "source": [
    "# Cell-penetrating peptides (CPP) prediction\n",
    "\n",
    "This notebook focus on linear peptides with all natural amino acids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3726f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e8db3f5",
   "metadata": {},
   "source": [
    "# CPP924 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a4dc41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cpp924 = pd.read_csv('data/CPP924/train.csv')\n",
    "df_test_cpp924 = pd.read_csv('data/CPP924/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c573f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train_cpp924.is_cpp.values\n",
    "y_test = df_test_cpp924.is_cpp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d0f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "def get_metrics(y_hat, y_test):\n",
    "    acc = accuracy_score(y_test, y_hat)\n",
    "    sn = recall_score(y_test, y_hat)\n",
    "    sp = recall_score(y_test, y_hat, pos_label=0)\n",
    "    mcc = matthews_corrcoef(y_test, y_hat)\n",
    "    auroc = roc_auc_score(y_test, y_hat)\n",
    "\n",
    "    print(f'Acc(%) \\t Sn(%) \\t Sp(%) \\t MCC \\t AUROC')\n",
    "    print(f'{acc*100:.2f}\\t{sn*100:.2f}\\t{sp*100:.2f}\\t{mcc:.3f}\\t{auroc:.3f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef2ea4ab",
   "metadata": {},
   "source": [
    "## Feature processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb2bd0e7",
   "metadata": {},
   "source": [
    "### Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd691b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from rdkit import Chem, rdBase, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from typing import List\n",
    "\n",
    "rdBase.DisableLog('rdApp.error')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def fingerprints_from_smiles(smiles: List, size=2048):\n",
    "    \"\"\"\n",
    "        Create ECFP fingerprints of smiles, with validity check\n",
    "    \"\"\"\n",
    "    fps = []\n",
    "    valid_mask = []\n",
    "    for i, smile in enumerate(smiles):\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        valid_mask.append(int(mol is not None))\n",
    "        fp = fingerprints_from_mol(mol, size=size) if mol else np.zeros((1, size))\n",
    "        fps.append(fp)\n",
    "\n",
    "    fps = np.concatenate(fps, axis=0)\n",
    "    return fps, valid_mask\n",
    "\n",
    "\n",
    "def fingerprints_from_mol(molecule, radius=3, size=2048, hashed=False):\n",
    "    \"\"\"\n",
    "        Create ECFP fingerprint of a molecule\n",
    "    \"\"\"\n",
    "    if hashed:\n",
    "        fp_bits = AllChem.GetHashedMorganFingerprint(molecule, radius, nBits=size)\n",
    "    else:\n",
    "        fp_bits = AllChem.GetMorganFingerprintAsBitVect(molecule, radius, nBits=size)\n",
    "    fp_np = np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(fp_bits, fp_np)\n",
    "    return fp_np.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07976b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((733, 2048), (733,), (183, 2048), (183,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = True\n",
    "if train:\n",
    "    X_train = fingerprints_from_smiles(df_train_cpp924.smi)[0]\n",
    "    X_test = fingerprints_from_smiles(df_test_cpp924.smi)[0]\n",
    "\n",
    "    np.save('data/CPP924/X_train_fps.npy', X_train)\n",
    "    np.save('data/CPP924/X_test_fps.npy', X_test)\n",
    "else:\n",
    "    X_train = np.load('data/CPP924/X_train_fps.npy')\n",
    "    X_test = np.load('data/CPP924/X_test_fps.npy')\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0d5137a",
   "metadata": {},
   "source": [
    "### ESM-2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8fe3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "def load_esm_model():\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D() # E=1280 90.71\t93.88\t87.06\t0.814\t0.905\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()  # disables dropout for deterministic results\n",
    "    return model, batch_converter, alphabet\n",
    "\n",
    "def get_esm_seq_representation(aa_seqs, batch_converter, model, alphabet, token_embd=\"mean\"):\n",
    "\n",
    "    data = [(f\"seq{id}\", seq) for id, seq in enumerate(aa_seqs)]\n",
    "    _, _, batch_tokens = batch_converter(data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[-1], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][-1]\n",
    "\n",
    "    if token_embd == \"mean\":\n",
    "        sequence_representations = []\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0)) # Take mean of non-pad tokens\n",
    "        \n",
    "        return torch.stack(sequence_representations).numpy()\n",
    "    else:\n",
    "        return token_representations.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c4d892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((733, 1280), (733,), (183, 1280), (183,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = True\n",
    "train = False\n",
    "if train:\n",
    "    model, batch_converter, alphabet = load_esm_model()\n",
    "    X_train = get_esm_seq_representation(df_train_cpp924.aa_seq, batch_converter, model, alphabet)\n",
    "    X_test = get_esm_seq_representation(df_test_cpp924.aa_seq, batch_converter, model, alphabet)\n",
    "\n",
    "    np.save('data/CPP924/X_train_esm2.npy', X_train)\n",
    "    np.save('data/CPP924/X_test_esm2.npy', X_test)\n",
    "else:\n",
    "    X_train = np.load('data/CPP924/X_train_esm2.npy')\n",
    "    X_test = np.load('data/CPP924/X_test_esm2.npy')\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f7dbbc4",
   "metadata": {},
   "source": [
    "### BERT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a13502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from utils.utils import parse_config, load_model, get_metrics\n",
    "from models.bert import BERT\n",
    "from datasets.tokenizer import SmilesTokenizer, AATokenizer\n",
    "\n",
    "def load_bert_model(ckpt, config, device='cuda', model_type='smi_bert'):\n",
    "    if model_type == 'smi_bert':\n",
    "        tokenizer = SmilesTokenizer(max_len=config.data.max_len)\n",
    "    elif model_type == 'aa_bert':\n",
    "        tokenizer = AATokenizer(max_len=config.data.max_len)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid model_type: {model_type}')\n",
    "\n",
    "    model = BERT(tokenizer, **config.model)\n",
    "    model = load_model(model, ckpt, device)\n",
    "    model.eval()\n",
    "    return model, device\n",
    "\n",
    "def get_bert_embd(encoder, inputs, device='cuda', cls_token=False):\n",
    "    tokens = encoder.tokenize_inputs(inputs).to(device)\n",
    "    embd = encoder.embed(tokens)\n",
    "    if cls_token:\n",
    "        del tokens\n",
    "        return embd[:, 0, :].squeeze()\n",
    "    else:\n",
    "        batch_lens = (tokens != encoder.tokenizer.pad_token_id).sum(1)\n",
    "        reps = []\n",
    "        for i, tokens_len in enumerate(batch_lens):\n",
    "            reps.append(embd[i, 1 : tokens_len - 1].mean(0))\n",
    "        del tokens, batch_lens, embd\n",
    "        return torch.stack(reps)\n",
    "\n",
    "def encode_with_bert(list, model, device='cuda', cls_token=False):\n",
    "    with torch.no_grad():\n",
    "        output= get_bert_embd(model, list, device=device, cls_token=cls_token)\n",
    "        embd = output.cpu().numpy()\n",
    "    return embd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab5f8960",
   "metadata": {},
   "source": [
    "#### SMILES BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81a327b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((733, 512), (733,), (183, 512), (183,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = True\n",
    "train = False\n",
    "if train:\n",
    "    ckpt='results/train_smi_bert_tune/model_13_0.003.pt'  # 90.71\t93.88\t87.06\t0.814\t0.905\n",
    "    config_file='configs/train_smi_bert_tune.yaml'\n",
    "    config = parse_config(config_file)\n",
    "    model, device = load_bert_model(ckpt=ckpt, config=config)\n",
    "\n",
    "    X_train = encode_with_bert(df_train_cpp924.smi, model)\n",
    "    X_test = encode_with_bert(df_test_cpp924.smi, model)\n",
    "\n",
    "    np.save('data/CPP924/X_train_smi_bert.npy', X_train)\n",
    "    np.save('data/CPP924/X_test_smi_bert.npy', X_test)\n",
    "else:\n",
    "    X_train = np.load('data/CPP924/X_train_smi_bert.npy')\n",
    "    X_test = np.load('data/CPP924/X_test_smi_bert.npy')\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d37c411f",
   "metadata": {},
   "source": [
    "#### AA BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f70bdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((733, 256), (733,), (183, 256), (183,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = True\n",
    "train = False\n",
    "if train:\n",
    "    ckpt='results/train_aa_bert/model_12_2.523.pt'  # 94.54\t97.96\t90.59\t0.892\t0.943\n",
    "    config_file='configs/train_aa_bert_test.yaml'\n",
    "    config = parse_config(config_file)\n",
    "    model, device = load_bert_model(ckpt=ckpt, config=config, model_type='aa_bert')\n",
    "\n",
    "    X_train = encode_with_bert(df_train_cpp924.aa_seq, model)\n",
    "    X_test = encode_with_bert(df_test_cpp924.aa_seq, model)\n",
    "\n",
    "    np.save('data/CPP924/X_train_aa_bert.npy', X_train)\n",
    "    np.save('data/CPP924/X_test_aa_bert.npy', X_test)\n",
    "else:\n",
    "    X_train = np.load('data/CPP924/X_train_aa_bert.npy')\n",
    "    X_test = np.load('data/CPP924/X_test_aa_bert.npy')\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7798433e",
   "metadata": {},
   "source": [
    "### MolClip features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccce3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "from models.molclip import MolCLIP\n",
    "from utils.utils import load_model, parse_config\n",
    "\n",
    "def load_molclip_model(ckpt, config, device='cuda'):\n",
    "    model = MolCLIP(device=device, config=config.model)\n",
    "    model = load_model(model, ckpt, device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_molclip_embd(model, smiles, aa_seq):\n",
    "    with torch.no_grad():\n",
    "        smi_output, _ = model.get_smi_embd(smiles)\n",
    "        smi_feat = model.smi_proj(smi_output)\n",
    "        aa_output, _ = model.get_aa_embd(aa_seq)\n",
    "        aa_feat = model.aa_proj(aa_output)\n",
    "        feat = torch.cat([smi_feat, aa_feat], dim=1)\n",
    "        embd = feat.cpu().numpy()\n",
    "    return embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18ed7917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((733, 512), (733,), (183, 512), (183,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = True\n",
    "train = False\n",
    "if train:\n",
    "    config = './configs/train_molclip_test.yaml'\n",
    "    # ckpt='results/train_molclip_test/model_0_0.000.pt'  # 88.52\t87.76\t89.41\t0.770\t0.886\n",
    "    ckpt = 'results/train_molclip/model_1_2.618.pt' # 87.98\t89.80\t85.88\t0.758\t0.878\n",
    "    model = load_molclip_model(ckpt=ckpt, config=parse_config(config))\n",
    "    \n",
    "    X_train = get_molclip_embd(model, df_train_cpp924.smi, df_train_cpp924.aa_seq)\n",
    "    X_test = get_molclip_embd(model, df_test_cpp924.smi, df_test_cpp924.aa_seq)\n",
    "    \n",
    "    np.save('data/CPP924/X_train_mol_clip.npy', X_train)\n",
    "    np.save('data/CPP924/X_test_mol_clip.npy', X_test)\n",
    "else:\n",
    "    X_train = np.load('data/CPP924/X_train_mol_clip.npy')\n",
    "    X_test = np.load('data/CPP924/X_test_mol_clip.npy')\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c7931",
   "metadata": {},
   "source": [
    "### BART features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36f1e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from utils.utils import parse_config, load_model, get_metrics\n",
    "from models.pbart import PepBART\n",
    "\n",
    "def load_pep_bart_model(ckpt, config, device='cuda'):\n",
    "\n",
    "    model = PepBART(device=device, config=config.model).to(device)\n",
    "    model = load_model(model, ckpt, device)\n",
    "    model.eval()\n",
    "    return model, device\n",
    "\n",
    "def get_pep_bart_embd(model, inputs, device='cuda'):\n",
    "    tokens = model.aa_encoder.tokenize_inputs(inputs).to(device)\n",
    "    embd = model.aa_encoder.embed(tokens)\n",
    "    batch_lens = (tokens != model.aa_encoder.tokenizer.pad_token_id).sum(1)\n",
    "    reps = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        reps.append(embd[i, 1 : tokens_len - 1].mean(0))\n",
    "    del tokens, batch_lens, embd\n",
    "    return torch.stack(reps)\n",
    "\n",
    "def encode_with_pep_bart(list, model, device='cuda'):\n",
    "    with torch.no_grad():\n",
    "        output= get_pep_bart_embd(model, list, device=device)\n",
    "        embd = output.cpu().numpy()\n",
    "    return embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cd7a244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((733, 256), (733,), (183, 256), (183,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = True\n",
    "# train = False\n",
    "if train:\n",
    "    ckpt='results/train_pep_bart/model_1_1.246.pt'\n",
    "    config_file='configs/train_pep_bart_test.yaml'\n",
    "    config = parse_config(config_file)\n",
    "    model, device = load_pep_bart_model(ckpt=ckpt, config=config)\n",
    "\n",
    "    X_train = encode_with_pep_bart(df_train_cpp924.aa_seq, model)\n",
    "    X_test = encode_with_pep_bart(df_test_cpp924.aa_seq, model)\n",
    "\n",
    "    np.save('data/CPP924/X_train_pep_bart.npy', X_train)\n",
    "    np.save('data/CPP924/X_test_pep_bart.npy', X_test)\n",
    "else:\n",
    "    X_train = np.load('data/CPP924/X_train_pep_bart.npy')\n",
    "    X_test = np.load('data/CPP924/X_test_pep_bart.npy')\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f08286c",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd8b4000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((733, 256), (733,), (183, 256), (183,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = ['fps', ]\n",
    "# features = ['esm2', ] \n",
    "# features = ['smi_bert', ]\n",
    "# features = ['aa_bert', ] \n",
    "# features = ['molclip', ] \n",
    "features = ['pep_bart', ] \n",
    "# features = ['esm2', 'fps', ]  \n",
    "# features = ['esm2', 'smi_bert', ]\n",
    "# features = ['esm2', 'fps', 'smi_bert']\n",
    "# features = ['smi_bert', 'aa_bert', ]\n",
    "\n",
    "X_train_features = []\n",
    "X_test_features = []\n",
    "for feat in features:\n",
    "    if feat == 'esm2':\n",
    "        X_train = np.load('data/CPP924/X_train_esm2.npy')\n",
    "        X_test = np.load('data/CPP924/X_test_esm2.npy')\n",
    "        \n",
    "        X_train_features.append(X_train)\n",
    "        X_test_features.append(X_test)\n",
    "    elif feat == 'fps':\n",
    "        X_train = np.load('data/CPP924/X_train_fps.npy')\n",
    "        X_test = np.load('data/CPP924/X_test_fps.npy')\n",
    "\n",
    "        X_train_features.append(X_train)\n",
    "        X_test_features.append(X_test)\n",
    "    elif feat == 'smi_bert':\n",
    "        X_train = np.load('data/CPP924/X_train_smi_bert.npy')\n",
    "        X_test = np.load('data/CPP924/X_test_smi_bert.npy')\n",
    "\n",
    "        X_train_features.append(X_train)\n",
    "        X_test_features.append(X_test)\n",
    "    elif feat == 'aa_bert':\n",
    "        X_train = np.load('data/CPP924/X_train_aa_bert.npy')\n",
    "        X_test = np.load('data/CPP924/X_test_aa_bert.npy')\n",
    "\n",
    "        X_train_features.append(X_train)\n",
    "        X_test_features.append(X_test)\n",
    "    elif feat == 'molclip':\n",
    "        X_train = np.load('data/CPP924/X_train_mol_clip.npy')\n",
    "        X_test = np.load('data/CPP924/X_test_mol_clip.npy')\n",
    "\n",
    "        X_train_features.append(X_train)\n",
    "        X_test_features.append(X_test)\n",
    "    elif feat == 'pep_bart':\n",
    "        X_train = np.load('data/CPP924/X_train_pep_bart.npy')\n",
    "        X_test = np.load('data/CPP924/X_test_pep_bart.npy')\n",
    "\n",
    "        X_train_features.append(X_train)\n",
    "        X_test_features.append(X_test)\n",
    "    else:\n",
    "        raise ValueError(f'Feature {feat} not supported')\n",
    "\n",
    "X_train = np.concatenate(X_train_features, axis=1)\n",
    "X_test = np.concatenate(X_test_features, axis=1)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1edcfd8",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "514f5075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc(%) \t Sn(%) \t Sp(%) \t MCC \t AUROC\n",
      "91.80\t93.88\t89.41\t0.835\t0.916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9180327868852459,\n",
       " 0.9387755102040817,\n",
       " 0.8941176470588236,\n",
       " 0.8353032703034405,\n",
       " 0.9164465786314526)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "get_metrics(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff9713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
