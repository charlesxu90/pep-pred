data: 
  input_path: 'data/smi_pretrain/'          # pretrain data folder, contain two files: train.csv and val.csv

tokenizer:  # Tokenizer params
  max_len: 189                         # *max length of sequences after tokenization, set a smaller number (relative to max_position_embeddings) to fit the training data and reduce memory usage

model:  # Model params
  vocab_size: 50265
  type_vocab_size: 1
  max_position_embeddings: 512           # max position embeddings of Transformer, sets the model size
  num_attention_heads: 12                 # number of attention heads in each hidden layer
  num_hidden_layers: 6                    # number of hidden layers
  hidden_dropout_prob: 0.1                # hidden layer dropout
  attention_probs_dropout_prob: 0.1       # attention dropout

dataloader:  # MLM params
  mlm_probability: 0.15                   # masked probability in mlm

train:  # Training params
  output_dir: 'checkpoints/pretrain'      # *logging and save path of the pretrained model
  overwrite_output_dir: True              # whether to overwrite output directory (i.e. True/False)
  num_train_epochs: 20                              # *total number of epochs
  per_device_train_batch_size: 150                         # *batch size
  per_device_eval_batch_size: 195                         # *batch size
  save_strategy: 'epoch'                  # save strategy of trainer
  save_total_limit: 3                     # save total limit of trainer
  fp16: True                              # float precision 16 (i.e. True/False)
  logging_strategy: 'epoch'               # logging frequency
  evaluation_strategy: 'epoch'            # validation frequency
  learning_rate: 0.00005                        # *learning rate
  lr_scheduler_type: 'linear'                # lr scheduler type
  weight_decay: 0.0                       # weight decay for AdamW
  warmup_ratio: 0.05                      # warm-up ratio for scheduler
  report_to: 'tensorboard'                # integrations to report the results and logs to
  dataloader_num_workers: 18              # Number of subprocesses to use for data loading
  sharded_ddp: False                      # option of Sharded DDP training

load_checkpoint: False


