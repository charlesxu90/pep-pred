data:
  input_path: 'data/PeptideAtlas/'  # pretrain data folder, contain two files: train.csv and val.csv
  col_name: 'smi,aa_seq'            # column name of SMILES and AA sequence
  max_len: 600                      # *max length after tokenization, set a smaller number (rf. max_position_embeddings) to reduce memory usage
  batch_size: 44                    # 2xA100 40GB 4.5h/epoch
  num_workers: 2                    # number of workers for data loading

model:  # Model params
  smi_bert:
    context_length: 600             # default 400
    width: 512
    n_heads: 8
    n_layers: 6
    mlm_probability: 0.15           # masked probability in mlm
  
  aa_bert:
    context_length: 256             # default 400
    width: 256
    n_heads: 8
    n_layers: 6
    mlm_probability: 0.15           # masked probability in mlm
  
  smi_max_len: 189
  aa_max_len: 40
  cls_token_embd: true
  proj_dim: 256
  temp_scale: 0.07
  esm: false

train:  # Training params
  max_epochs: 5                     # *total number of epochs
  learning_rate: 0.0004             # *learning rate
  device: 'cuda'                    # device to use for training
  use_amp: true                     # whether to use automatic mixed precision training
